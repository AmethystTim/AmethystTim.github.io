## 霍夫丁不等式(Hoeffding's Inequality)

$$P(D_{train} \quad is \quad bad \quad due \quad to \quad h)≤ 2exp(-2N\epsilon^2)$$

假如拥有一个较大的$|\mathbf{H}|$虽然可以得到更低的理论损失函数值但是训练集得到的损失函数值可能与理论偏差更大，反之亦然

<figure markdown>
![img](https://github.com/DINOREXNB/DINOREXNB.github.io/blob/main/docs/images/ml2-1.png?raw=true){width=400}
</figure>

如何得到$h^{all}=arg \min_{h\in H}L(h,D_{all})$这种理想情况？

## 为什么是"deep" learning而非"wide" learning?

假如构造一个分段线性函数，$a_1=1\cdot \max(0,x-0.5)+1\cdot \max(0,-x+0.5)$

<figure markdown>
![img](https://github.com/DINOREXNB/DINOREXNB.github.io/blob/main/docs/images/ml2-2.png?raw=true){width=400}
</figure>